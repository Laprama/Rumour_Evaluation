{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('data_extraction/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reddit_data_extraction import *\n",
    "from twitter_data_extraction import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_zip = ('data/rumoureval-2019-training-data.zip')\n",
    "training_data_directory = ZipFile(training_data_zip)\n",
    "training_data_contents = get_directory_structure(training_data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_train_data = training_data_contents['reddit-training-data']\n",
    "reddit_dev_data = training_data_contents['reddit-dev-data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_english = training_data_contents['twitter-english']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = json.loads(training_data_directory.read('rumoureval-2019-training-data/train-key.json'))\n",
    "dev_labels = json.loads(training_data_directory.read('rumoureval-2019-training-data/dev-key.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reddit - Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev, train = get_train_dev(training_data_directory, reddit_train_data, reddit_dev_data, train_labels, dev_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_set = [d.pop('data', None) for d in dev]\n",
    "train_set = [d.pop('data', None) for d in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "development = pd.DataFrame(dev_set)\n",
    "training = pd.DataFrame(train_set)\n",
    "#training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter - Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   text  depth  split  \\\n",
       "0     MT @euronews France: 10 dead after shooting at...      1  train   \n",
       "1     @j0nathandavis They who? Stupid and partial op...      2  train   \n",
       "2     @nanoSpawn Socialists, Antisemites, anti zioni...      3  train   \n",
       "3     @euronews @TradeDesk_Steve A French crime of p...      1  train   \n",
       "4     @euronews LOL. 5 million Muslims in France, wh...      1  train   \n",
       "...                                                 ...    ...    ...   \n",
       "5563                         @emaccaz_ omfg it is ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±      2  train   \n",
       "5564  @Angus_OL thank god they're all safe now. some...      1  train   \n",
       "5565  @Angus_OL thank god its over, they're finally ...      1  train   \n",
       "5566                                       @Angus_OL :(      1  train   \n",
       "5567  Police confirm that #sydneysiege is finally ov...      0  train   \n",
       "\n",
       "     task_A_label  \n",
       "0         comment  \n",
       "1            deny  \n",
       "2         comment  \n",
       "3           query  \n",
       "4         comment  \n",
       "...           ...  \n",
       "5563      comment  \n",
       "5564      support  \n",
       "5565      comment  \n",
       "5566      comment  \n",
       "5567      support  \n",
       "\n",
       "[5568 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>depth</th>\n      <th>split</th>\n      <th>task_A_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>MT @euronews France: 10 dead after shooting at...</td>\n      <td>1</td>\n      <td>train</td>\n      <td>comment</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>@j0nathandavis They who? Stupid and partial op...</td>\n      <td>2</td>\n      <td>train</td>\n      <td>deny</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>@nanoSpawn Socialists, Antisemites, anti zioni...</td>\n      <td>3</td>\n      <td>train</td>\n      <td>comment</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>@euronews @TradeDesk_Steve A French crime of p...</td>\n      <td>1</td>\n      <td>train</td>\n      <td>query</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>@euronews LOL. 5 million Muslims in France, wh...</td>\n      <td>1</td>\n      <td>train</td>\n      <td>comment</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5563</th>\n      <td>@emaccaz_ omfg it is ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±ðŸ˜±</td>\n      <td>2</td>\n      <td>train</td>\n      <td>comment</td>\n    </tr>\n    <tr>\n      <th>5564</th>\n      <td>@Angus_OL thank god they're all safe now. some...</td>\n      <td>1</td>\n      <td>train</td>\n      <td>support</td>\n    </tr>\n    <tr>\n      <th>5565</th>\n      <td>@Angus_OL thank god its over, they're finally ...</td>\n      <td>1</td>\n      <td>train</td>\n      <td>comment</td>\n    </tr>\n    <tr>\n      <th>5566</th>\n      <td>@Angus_OL :(</td>\n      <td>1</td>\n      <td>train</td>\n      <td>comment</td>\n    </tr>\n    <tr>\n      <th>5567</th>\n      <td>Police confirm that #sydneysiege is finally ov...</td>\n      <td>0</td>\n      <td>train</td>\n      <td>support</td>\n    </tr>\n  </tbody>\n</table>\n<p>5568 rows Ã— 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "data = get_twitter_data(training_data_directory, twitter_english, train_labels, dev_labels)\n",
    "df = pd.DataFrame(data)\n",
    "df[['text', 'depth', 'split', 'task_A_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading the model\n",
      "Done!\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'vec_labels' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-af7db90d62da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# add each element of the vector as an individual features to the dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdata_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minsert_embedding_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/IAI_CDT/TB1/Dialogue_and_Narrative/Assessment/rumour_detection/Rumour_Evaluation/data_extraction/embeddings.py\u001b[0m in \u001b[0;36minsert_embedding_feature\u001b[0;34m(data_df, vec_array)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# add each element of the vector as an individual features to the dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minsert_embedding_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mdata_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvec_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vec_labels' is not defined"
     ]
    }
   ],
   "source": [
    "from embeddings import *\n",
    "\n",
    "data_df = df.copy()\n",
    "\n",
    "# Process Tweets\n",
    "data_df = get_cleaned_tweets(data_df)\n",
    "\n",
    "# Load W2V model\n",
    "model = loadW2vModel()\n",
    "\n",
    "# Create list of columns for each feature\n",
    "# Create list of columns for each feature\n",
    "vec_ID = list(range(1,301))\n",
    "vec_labels = ['V{}'.format(i) for i in vec_ID]\n",
    "\n",
    "vec_array = get_embeddings(data_df, model, vec_labels)\n",
    "\n",
    "# add each element of the vector as an individual features to the dataframe\n",
    "data_df = insert_embedding_feature(data_df, vec_array, vec_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('nlp_env')",
   "metadata": {
    "interpreter": {
     "hash": "d6e1b16cf08279d9584d1a050bb34f5e80b2af703052209dff1f21a6f734618e"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}